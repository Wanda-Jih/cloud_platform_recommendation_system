{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation Engine for E-Commerce Sales: Part 1. Data Preparation\n",
    "\n",
    "This notebook gives an overview of techniques and services offer by SageMaker to build and deploy a personalized recommendation engine.\n",
    "\n",
    "## Dataset\n",
    "\n",
    "The dataset for this demo comes from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets/Online+Retail). It contains all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered non-store online retail. The company mainly sells unique all-occasion gifts. The following attributes are included in our dataset:\n",
    "+ InvoiceNo: Invoice number. Nominal, a 6-digit integral number uniquely assigned to each transaction. If this code starts with letter 'c', it indicates a cancellation.\n",
    "+ StockCode: Product (item) code. Nominal, a 5-digit integral number uniquely assigned to each distinct product.\n",
    "+ Description: Product (item) name. Nominal.\n",
    "+ Quantity: The quantities of each product (item) per transaction. Numeric.\n",
    "+ InvoiceDate: Invice Date and time. Numeric, the day and time when each transaction was generated.\n",
    "+ UnitPrice: Unit price. Numeric, Product price per unit in sterling.\n",
    "+ CustomerID: Customer number. Nominal, a 5-digit integral number uniquely assigned to each customer.\n",
    "+ Country: Country name. Nominal, the name of the country where each customer resides. \n",
    "\n",
    "Citation: Daqing Chen, Sai Liang Sain, and Kun Guo, Data mining for the online retail industry: A case study of RFM model-based customer segmentation using data mining, Journal of Database Marketing and Customer Strategy Management, Vol. 19, No. 3, pp. 197â€“208, 2012 (Published online before print: 27 August 2012. doi: 10.1057/dbm.2012.17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution Architecture\n",
    "----\n",
    "![Architecture](./images/retail_rec_dataprep.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Python 2.7 reached the end of its life on January 1st, 2020. Please upgrade your Python as Python 2.7 is no longer maintained. pip 21.0 will drop support for Python 2.7 in January 2021. More details about Python 2 support in pip can be found at https://pip.pypa.io/en/latest/development/release-process/#python-2-support pip 21.0 will remove support for this functionality.\u001b[0m\n",
      "Requirement already satisfied: sagemaker in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (2.86.2)\n",
      "Requirement already satisfied: boto3 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (1.17.112)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from sagemaker) (0.24.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from sagemaker) (20.9)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from sagemaker) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from sagemaker) (3.11.2)\n",
      "Requirement already satisfied: attrs==20.3.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from sagemaker) (20.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=1.4.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from sagemaker) (2.1.2)\n",
      "Requirement already satisfied: smdebug-rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from sagemaker) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.9.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from sagemaker) (1.16.5)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from sagemaker) (0.2.8)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from sagemaker) (0.1.5)\n",
      "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from boto3) (0.4.2)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from boto3) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.21.0,>=1.20.112 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from boto3) (1.20.112)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from pandas->sagemaker) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from pandas->sagemaker) (2019.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from packaging>=20.0->sagemaker) (2.4.2)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from google-pasta->sagemaker) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from protobuf>=3.1->sagemaker) (41.4.0)\n",
      "Requirement already satisfied: pathlib2; python_version < \"3\" in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (2.3.5)\n",
      "Requirement already satisfied: contextlib2; python_version < \"3\" in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (0.6.0)\n",
      "Requirement already satisfied: configparser>=3.5; python_version < \"3\" in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from importlib-metadata>=1.4.0->sagemaker) (4.0.2)\n",
      "Requirement already satisfied: ppft>=1.6.6.4 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from pathos->sagemaker) (1.6.6.4)\n",
      "Requirement already satisfied: multiprocess>=0.70.12 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from pathos->sagemaker) (0.70.12.2)\n",
      "Requirement already satisfied: pox>=0.3.0 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from pathos->sagemaker) (0.3.0)\n",
      "Requirement already satisfied: dill>=0.3.4 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from pathos->sagemaker) (0.3.4)\n",
      "Requirement already satisfied: futures<4.0.0,>=2.2.0; python_version == \"2.7\" in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from s3transfer<0.5.0,>=0.4.0->boto3) (3.3.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from botocore<1.21.0,>=1.20.112->boto3) (1.25.11)\n",
      "Requirement already satisfied: scandir; python_version < \"3.5\" in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from pathlib2; python_version < \"3\"->importlib-metadata>=1.4.0->sagemaker) (1.10.0)\n",
      "Requirement already satisfied: more-itertools in /home/ec2-user/anaconda3/envs/amazonei_mxnet_p27/lib/python2.7/site-packages (from zipp>=0.5->importlib-metadata>=1.4.0->sagemaker) (5.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sagemaker boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store command \n",
    "For saving checkpoint through our notebook. In this way, if we ever terminate the kernel, but then later need to pick back up in the notebook, some of our variables will be stored for us already.\n",
    "For example, using these commands, we will not need to fit our estimator again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored variables and their in-db values:\n"
     ]
    }
   ],
   "source": [
    "%store -r\n",
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "import sagemaker.amazon.common as smac\n",
    "from sagemaker.lineage import context, artifact, association, action\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import CreateModelStep, ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterFloat, ParameterString\n",
    "\n",
    "from model_package_src.inference_specification import InferenceSpecification\n",
    "import io\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "from scipy.sparse import csr_matrix, hstack, save_npz\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert sagemaker.__version__ >= \"2.21.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure our SageMaker clent and session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using bucketsagemaker-us-east-1-267710284436 in region us-east-1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "region = boto3.Session().region_name\n",
    "boto3.setup_default_session(region_name=region)\n",
    "boto_session = boto3.Session(region_name=region)\n",
    "\n",
    "s3_client = boto3.client(\"s3\", region_name=region)\n",
    "\n",
    "sagemaker_boto_client = boto_session.client(\"sagemaker\")\n",
    "sagemaker_session = sagemaker.session.Session(\n",
    "    boto_session=boto_session, sagemaker_client=sagemaker_boto_client\n",
    ")\n",
    "sagemaker_role = sagemaker.get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "print(f\"using bucket{bucket} in region {region} \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(541909, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>InvoiceNo</th>\n",
       "      <th>StockCode</th>\n",
       "      <th>Description</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>InvoiceDate</th>\n",
       "      <th>UnitPrice</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Country</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>536365</td>\n",
       "      <td>85123A</td>\n",
       "      <td>WHITE HANGING HEART T-LIGHT HOLDER</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.55</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>536365</td>\n",
       "      <td>71053</td>\n",
       "      <td>WHITE METAL LANTERN</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>536365</td>\n",
       "      <td>84406B</td>\n",
       "      <td>CREAM CUPID HEARTS COAT HANGER</td>\n",
       "      <td>8</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>2.75</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029G</td>\n",
       "      <td>KNITTED UNION FLAG HOT WATER BOTTLE</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>536365</td>\n",
       "      <td>84029E</td>\n",
       "      <td>RED WOOLLY HOTTIE WHITE HEART.</td>\n",
       "      <td>6</td>\n",
       "      <td>2010-12-01 08:26:00</td>\n",
       "      <td>3.39</td>\n",
       "      <td>17850.0</td>\n",
       "      <td>United Kingdom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  InvoiceNo StockCode                          Description  Quantity  \\\n",
       "0    536365    85123A   WHITE HANGING HEART T-LIGHT HOLDER         6   \n",
       "1    536365     71053                  WHITE METAL LANTERN         6   \n",
       "2    536365    84406B       CREAM CUPID HEARTS COAT HANGER         8   \n",
       "3    536365    84029G  KNITTED UNION FLAG HOT WATER BOTTLE         6   \n",
       "4    536365    84029E       RED WOOLLY HOTTIE WHITE HEART.         6   \n",
       "\n",
       "           InvoiceDate  UnitPrice  CustomerID         Country  \n",
       "0  2010-12-01 08:26:00       2.55     17850.0  United Kingdom  \n",
       "1  2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
       "2  2010-12-01 08:26:00       2.75     17850.0  United Kingdom  \n",
       "3  2010-12-01 08:26:00       3.39     17850.0  United Kingdom  \n",
       "4  2010-12-01 08:26:00       3.39     17850.0  United Kingdom  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/Online Retail.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "\n",
    "First, we check for any null (i.e. missing) values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "InvoiceNo           0\n",
       "StockCode           0\n",
       "Description      1454\n",
       "Quantity            0\n",
       "InvoiceDate         0\n",
       "UnitPrice           0\n",
       "CustomerID     135080\n",
       "Country             0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop any records with a missing CustomerID. If we do not know who the customer is, then it is not helpful to us when we make recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406829, 8)\n"
     ]
    }
   ],
   "source": [
    "df.dropna(subset=[\"CustomerID\"], inplace=True)\n",
    "df[\"Description\"] = df[\"Description\"].apply(lambda x: x.strip())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there's is any negative value for UnitPrice. But it still has some extreme outliers.\n",
    "We're not gonna do anything to handle that because items with high prices are relatively normal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df[\"UnitPrice\"], kde=True)\n",
    "plt.title(\"Distribution of Unit Prices\")\n",
    "plt.xlabel(\"Price\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see a large spike around zero, meaning small quantity, but we do also see vary large negative and large postive quantities as well. These negative quantites might not make intuitive sense, but it turns out that that's how cancellations or return are handled. In this case, it looks like someone placed a very large order of 80,000 units, and then that was subsequently cancelled by 80,000 negative quantity.\n",
    "We should handle with the extreme case, so let's aggregate the data over the year and take the sum of the quantity. Therefore any transaction that are associated with a cancellation or a refund will go away. Also, we can make sure that our final quantities are greater than zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/seaborn/distributions.py:2557: FutureWarning: `distplot` is a deprecated function and will be removed in a future version. Please adapt your code to use either `displot` (a figure-level function with similar flexibility) or `histplot` (an axes-level function for histograms).\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.distplot(df[\"Quantity\"], kde=True)\n",
    "plt.title(\"Distribution of Quantity\")\n",
    "plt.xlabel(\"Quantity\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of our quantities are realteively small (positive) numbers, but there are also some negative quantities as well as extreme outliers (both postiive and negative outliers). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no negative prices, which is good, but we can see some extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(274399, 6)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.groupby([\"StockCode\", \"Description\", \"CustomerID\", \"Country\", \"UnitPrice\"])[\"Quantity\"].sum()\n",
    "df = df.loc[df > 0].reset_index()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we still have some categorical features, and we will need to handle them before training a model.\n",
    "Handling these categorical columns will create many additional columns, resulting in sparse data.\n",
    "Using sparse matrices can allow us to store all the data without running into memory error.\n",
    "\n",
    "Since the description column is text-based, we can apply text featurization to it. The method that we will use is TD-IDF, which stands for Term Frequency-Inverse Document Frequency. TD-IDF measures how relavant that term is to the description compared to how relavant it is for all descriptions. \n",
    "For example, if we have a descroption of the product \"White Metal Lantern\". The output of our text featurization will give us a column for each of those words and the values we weighted based off how frequent those words are in other description.\n",
    "\n",
    "For the \"StockCode\", \"CustomerID\", \"Country\", we will one-hot encode those. These will leave us with very sparse data since most users only buy a small subset of all products. Factorization Machines handles sparse data very well, and it designs for users and products to be one-hot encoded this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(dataframe):\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    onehot_cols = [\"StockCode\", \"CustomerID\", \"Country\"]\n",
    "    ohe_output = enc.fit_transform(dataframe[onehot_cols])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=2)\n",
    "    unique_descriptions = dataframe[\"Description\"].unique()\n",
    "    vectorizer.fit(unique_descriptions)\n",
    "    tfidf_output = vectorizer.transform(dataframe[\"Description\"])\n",
    "\n",
    "    row = range(len(dataframe))\n",
    "    col = [0] * len(dataframe)\n",
    "    unit_price = csr_matrix((dataframe[\"UnitPrice\"].values, (row, col)), dtype=\"float32\")\n",
    "\n",
    "    X = hstack([ohe_output, tfidf_output, unit_price], format=\"csr\", dtype=\"float32\")\n",
    "\n",
    "    y = dataframe[\"Quantity\"].values.astype(\"float32\")\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = loadDataset(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991284988048746"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display sparsity\n",
    "total_cells = X.shape[0] * X.shape[1]\n",
    "(total_cells - X.nnz) / total_cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is over 99.9% sparse. Because of this high sparsity, the sparse matrix data type allows us to represent our data using only a small fraction of the memory that a dense matrix would require.\n",
    "\n",
    "In the end, we left a sparse matrix where the rows are each product that the user brought. And our columns are all of our features. Don't worry, Factorization Machines will handle high-sparse data very well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Data For Modeling\n",
    "\n",
    "+ Split the data into training and testing sets\n",
    "+ Write the data to protobuf recordIO format for Pipe mode. [Read more](https://docs.aws.amazon.com/sagemaker/latest/dg/cdf-training.html) about protobuf recordIO format. Using this is because Factorization Machines take the protobuf  recordIO format as an input. Using this format allow you to take advantage of pipe mode. In pipe mode, your training data streams data directly from s3. Streaming data can provide faster start times for training jobs and better throughput.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((219519, 9284), (54880, 9284), (219519,), (54880,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save numpy arrays to local storage in /data folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"data/online_retail_preprocessed.csv\", index=False)\n",
    "# save_npz(\"data/X_train.npz\", X_train)\n",
    "# save_npz(\"data/X_test.npz\", X_test)\n",
    "# np.savez(\"data/y_train.npz\", y_train)\n",
    "# np.savez(\"data/y_test.npz\", y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"personalization\"\n",
    "\n",
    "train_key = \"train.protobuf\"\n",
    "train_prefix = f\"{prefix}/train\"\n",
    "\n",
    "test_key = \"test.protobuf\"\n",
    "test_prefix = f\"{prefix}/test\"\n",
    "\n",
    "output_prefix = f\"s3://{bucket}/{prefix}/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-us-east-1-267710284436/personalization/train/train.protobuf\n",
      "s3://sagemaker-us-east-1-267710284436/personalization/test/test.protobuf\n",
      "Output: s3://sagemaker-us-east-1-267710284436/personalization/output\n"
     ]
    }
   ],
   "source": [
    "def writeDatasetToProtobuf(X, y, bucket, prefix, key):\n",
    "    buf = io.BytesIO()\n",
    "    smac.write_spmatrix_to_sparse_tensor(buf, X, y)\n",
    "    buf.seek(0)\n",
    "    obj = \"{}/{}\".format(prefix, key)\n",
    "    boto3.resource(\"s3\").Bucket(bucket).Object(obj).upload_fileobj(buf)\n",
    "    return \"s3://{}/{}\".format(bucket, obj)\n",
    "\n",
    "\n",
    "train_data_location = writeDatasetToProtobuf(X_train, y_train, bucket, train_prefix, train_key)\n",
    "test_data_location = writeDatasetToProtobuf(X_test, y_test, bucket, test_prefix, test_key)\n",
    "\n",
    "print(train_data_location)\n",
    "print(test_data_location)\n",
    "print(\"Output: {}\".format(output_prefix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %store train_data_location\n",
    "# %store test_data_location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the factorization machine model\n",
    "\n",
    "Once we have the data preprocessed and available in the correct format for training, the next step is to actually train the model using the data. \n",
    "\n",
    "We'll use the Amazon SageMaker Python SDK to kick off training and monitor status until it is completed. In this example that takes only a few minutes. Despite the model only need 1-2 minutes to train, there is some extra time required upfront to provision hardware and load the algorithm container.\n",
    "\n",
    "First, let's specify our containers. To find the rigth container, we'll create a small lookup. More details on algorithm containers can be found in [AWS documentation.](https://docs-aws.amazon.com/sagemaker/latest/dg/sagemaker-algo-docker-registry-paths.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = sagemaker.image_uris.retrieve(\"factorization-machines\", region=boto_session.region_name)\n",
    "\n",
    "fm = sagemaker.estimator.Estimator(\n",
    "    container,\n",
    "    sagemaker_role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.c5.xlarge\",\n",
    "    output_path=output_prefix,\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "fm.set_hyperparameters(\n",
    "    feature_dim=X_train.shape[1],\n",
    "    predictor_type=\"regressor\",\n",
    "    mini_batch_size=1000,\n",
    "    num_factors=64,\n",
    "    epochs=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-04-15 19:41:57 Starting - Starting the training job...\n",
      "2022-04-15 19:42:22 Starting - Preparing the instances for trainingProfilerReport-1650051717: InProgress\n",
      "......\n",
      "2022-04-15 19:43:25 Downloading - Downloading input data...\n",
      "2022-04-15 19:43:42 Training - Downloading the training image.....\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:87: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.7/site-packages/algorithm/network_builder.py:120: DeprecationWarning: invalid escape sequence \\s\n",
      "  \"\"\"\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728 integration.py:636] worker started\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-conf.json: {'epochs': 1, 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0'}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'epochs': '20', 'feature_dim': '9284', 'mini_batch_size': '1000', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] Final configuration: {'epochs': '20', 'mini_batch_size': '1000', 'use_bias': 'true', 'use_linear': 'true', 'bias_lr': '0.1', 'linear_lr': '0.001', 'factors_lr': '0.0001', 'bias_wd': '0.01', 'linear_wd': '0.001', 'factors_wd': '0.00001', 'bias_init_method': 'normal', 'bias_init_sigma': '0.01', 'linear_init_method': 'normal', 'linear_init_sigma': '0.01', 'factors_init_method': 'normal', 'factors_init_sigma': '0.001', 'batch_metrics_publish_interval': '500', '_data_format': 'record', '_kvstore': 'auto', '_learning_rate': '1.0', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_optimizer': 'adam', '_tuning_objective_metric': '', '_use_full_symbolic': 'true', '_wd': '1.0', 'feature_dim': '9284', 'num_factors': '64', 'predictor_type': 'regressor'}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 WARNING 139933643601728] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] Using default worker.\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:44:42.131] [tensorio] [warning] TensorIO is already initialized; ignoring the initialization routine.\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:44:42.138] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 19, \"num_examples\": 1, \"num_bytes\": 99840}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] nvidia-smi: took 0.037 seconds to run.\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] [Sparse network] Building a sparse network.\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051882.1181216, \"EndTime\": 1650051882.182475, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 46.02336883544922, \"count\": 1, \"min\": 46.02336883544922, \"max\": 46.02336883544922}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051882.1826186, \"EndTime\": 1650051882.1830163, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 1000.0, \"count\": 1, \"min\": 1000, \"max\": 1000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Batches Since Last Reset\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}}}\u001b[0m\n",
      "\u001b[34m[19:44:42] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.206339.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[19:44:42] /opt/brazil-pkg-cache/packages/AIAlgorithmsMXNet/AIAlgorithmsMXNet-1.1.x.206339.0/AL2_x86_64/generic-flavor/src/src/kvstore/./kvstore_local.h:280: Warning: non-default weights detected during kvstore pull. This call has been ignored. Please make sure to use row_sparse_pull with row_ids.\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] #quality_metric: host=algo-1, epoch=0, batch=0 train rmse <loss>=72.22661905419636\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] #quality_metric: host=algo-1, epoch=0, batch=0 train mse <loss>=5216.6845\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:42 INFO 139933643601728] #quality_metric: host=algo-1, epoch=0, batch=0 train absolute_loss <loss>=17.63289453125\u001b[0m\n",
      "\n",
      "2022-04-15 19:44:42 Training - Training image download completed. Training in progress.\u001b[34m[2022-04-15 19:44:45.269] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 2, \"duration\": 3038, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:45 INFO 139933643601728] #quality_metric: host=algo-1, epoch=0, train rmse <loss>=91.21242742344528\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:45 INFO 139933643601728] #quality_metric: host=algo-1, epoch=0, train mse <loss>=8319.706916477273\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:45 INFO 139933643601728] #quality_metric: host=algo-1, epoch=0, train absolute_loss <loss>=16.162756942471592\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051882.182559, \"EndTime\": 1650051885.2701182, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"update.time\": {\"sum\": 3086.7276191711426, \"count\": 1, \"min\": 3086.7276191711426, \"max\": 3086.7276191711426}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:45 INFO 139933643601728] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051882.1833622, \"EndTime\": 1650051885.270404, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 220519.0, \"count\": 1, \"min\": 220519, \"max\": 220519}, \"Total Batches Seen\": {\"sum\": 221.0, \"count\": 1, \"min\": 221, \"max\": 221}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 2.0, \"count\": 1, \"min\": 2, \"max\": 2}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:45 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=71106.58841891633 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:45 INFO 139933643601728] #quality_metric: host=algo-1, epoch=1, batch=0 train rmse <loss>=70.0600956322499\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:45 INFO 139933643601728] #quality_metric: host=algo-1, epoch=1, batch=0 train mse <loss>=4908.417\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:45 INFO 139933643601728] #quality_metric: host=algo-1, epoch=1, batch=0 train absolute_loss <loss>=17.41325390625\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:44:48.069] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 4, \"duration\": 2797, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:48 INFO 139933643601728] #quality_metric: host=algo-1, epoch=1, train rmse <loss>=90.60802512809579\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:48 INFO 139933643601728] #quality_metric: host=algo-1, epoch=1, train mse <loss>=8209.814217613637\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:48 INFO 139933643601728] #quality_metric: host=algo-1, epoch=1, train absolute_loss <loss>=18.42056477716619\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051885.270198, \"EndTime\": 1650051888.0700104, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2799.302816390991, \"count\": 1, \"min\": 2799.302816390991, \"max\": 2799.302816390991}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:48 INFO 139933643601728] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051885.2706811, \"EndTime\": 1650051888.070244, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 1, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 440038.0, \"count\": 1, \"min\": 440038, \"max\": 440038}, \"Total Batches Seen\": {\"sum\": 441.0, \"count\": 1, \"min\": 441, \"max\": 441}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 3.0, \"count\": 1, \"min\": 3, \"max\": 3}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:48 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=78408.4036761781 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:48 INFO 139933643601728] #quality_metric: host=algo-1, epoch=2, batch=0 train rmse <loss>=69.8893983376592\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:48 INFO 139933643601728] #quality_metric: host=algo-1, epoch=2, batch=0 train mse <loss>=4884.528\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:48 INFO 139933643601728] #quality_metric: host=algo-1, epoch=2, batch=0 train absolute_loss <loss>=18.55230078125\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:44:50.846] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 6, \"duration\": 2774, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:50 INFO 139933643601728] #quality_metric: host=algo-1, epoch=2, train rmse <loss>=90.47375148602242\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:50 INFO 139933643601728] #quality_metric: host=algo-1, epoch=2, train mse <loss>=8185.499707954546\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:50 INFO 139933643601728] #quality_metric: host=algo-1, epoch=2, train absolute_loss <loss>=18.580566246448864\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051888.07008, \"EndTime\": 1650051890.847076, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2776.5417098999023, \"count\": 1, \"min\": 2776.5417098999023, \"max\": 2776.5417098999023}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:50 INFO 139933643601728] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051888.0705123, \"EndTime\": 1650051890.8472264, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 2, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 659557.0, \"count\": 1, \"min\": 659557, \"max\": 659557}, \"Total Batches Seen\": {\"sum\": 661.0, \"count\": 1, \"min\": 661, \"max\": 661}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 4.0, \"count\": 1, \"min\": 4, \"max\": 4}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:50 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=79054.62208564622 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:50 INFO 139933643601728] #quality_metric: host=algo-1, epoch=3, batch=0 train rmse <loss>=69.72750533326142\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:50 INFO 139933643601728] #quality_metric: host=algo-1, epoch=3, batch=0 train mse <loss>=4861.925\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:50 INFO 139933643601728] #quality_metric: host=algo-1, epoch=3, batch=0 train absolute_loss <loss>=18.33094140625\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:44:53.640] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 8, \"duration\": 2792, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:53 INFO 139933643601728] #quality_metric: host=algo-1, epoch=3, train rmse <loss>=90.33190701046739\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:53 INFO 139933643601728] #quality_metric: host=algo-1, epoch=3, train mse <loss>=8159.853424147727\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:53 INFO 139933643601728] #quality_metric: host=algo-1, epoch=3, train absolute_loss <loss>=18.27153690962358\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051890.84713, \"EndTime\": 1650051893.6413207, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2793.912410736084, \"count\": 1, \"min\": 2793.912410736084, \"max\": 2793.912410736084}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:53 INFO 139933643601728] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051890.8473883, \"EndTime\": 1650051893.6414692, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 3, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 879076.0, \"count\": 1, \"min\": 879076, \"max\": 879076}, \"Total Batches Seen\": {\"sum\": 881.0, \"count\": 1, \"min\": 881, \"max\": 881}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 5.0, \"count\": 1, \"min\": 5, \"max\": 5}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:53 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=78563.43325448799 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:53 INFO 139933643601728] #quality_metric: host=algo-1, epoch=4, batch=0 train rmse <loss>=69.57381332081778\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:53 INFO 139933643601728] #quality_metric: host=algo-1, epoch=4, batch=0 train mse <loss>=4840.5155\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:53 INFO 139933643601728] #quality_metric: host=algo-1, epoch=4, batch=0 train absolute_loss <loss>=18.07842578125\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:44:56.421] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 10, \"duration\": 2778, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:56 INFO 139933643601728] #quality_metric: host=algo-1, epoch=4, train rmse <loss>=90.2005097842629\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:56 INFO 139933643601728] #quality_metric: host=algo-1, epoch=4, train mse <loss>=8136.131965340909\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:56 INFO 139933643601728] #quality_metric: host=algo-1, epoch=4, train absolute_loss <loss>=18.01931514115767\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051893.6413755, \"EndTime\": 1650051896.4223206, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2780.3685665130615, \"count\": 1, \"min\": 2780.3685665130615, \"max\": 2780.3685665130615}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:56 INFO 139933643601728] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051893.6419284, \"EndTime\": 1650051896.4225104, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 4, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1098595.0, \"count\": 1, \"min\": 1098595, \"max\": 1098595}, \"Total Batches Seen\": {\"sum\": 1101.0, \"count\": 1, \"min\": 1101, \"max\": 1101}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 6.0, \"count\": 1, \"min\": 6, \"max\": 6}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:56 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=78944.67196874622 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:56 INFO 139933643601728] #quality_metric: host=algo-1, epoch=5, batch=0 train rmse <loss>=69.4391388195447\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:56 INFO 139933643601728] #quality_metric: host=algo-1, epoch=5, batch=0 train mse <loss>=4821.794\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:56 INFO 139933643601728] #quality_metric: host=algo-1, epoch=5, batch=0 train absolute_loss <loss>=17.880751953125\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:44:59.154] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 12, \"duration\": 2730, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:59 INFO 139933643601728] #quality_metric: host=algo-1, epoch=5, train rmse <loss>=90.08360983045266\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:59 INFO 139933643601728] #quality_metric: host=algo-1, epoch=5, train mse <loss>=8115.056760085227\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:59 INFO 139933643601728] #quality_metric: host=algo-1, epoch=5, train absolute_loss <loss>=17.845939994673294\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051896.4223738, \"EndTime\": 1650051899.1549852, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2732.28120803833, \"count\": 1, \"min\": 2732.28120803833, \"max\": 2732.28120803833}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:59 INFO 139933643601728] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051896.422678, \"EndTime\": 1650051899.155242, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 5, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1318114.0, \"count\": 1, \"min\": 1318114, \"max\": 1318114}, \"Total Batches Seen\": {\"sum\": 1321.0, \"count\": 1, \"min\": 1321, \"max\": 1321}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 7.0, \"count\": 1, \"min\": 7, \"max\": 7}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:59 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=80331.21014931076 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:59 INFO 139933643601728] #quality_metric: host=algo-1, epoch=6, batch=0 train rmse <loss>=69.32097085298214\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:59 INFO 139933643601728] #quality_metric: host=algo-1, epoch=6, batch=0 train mse <loss>=4805.397\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:44:59 INFO 139933643601728] #quality_metric: host=algo-1, epoch=6, batch=0 train absolute_loss <loss>=17.763322265625\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:02.102] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 14, \"duration\": 2945, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:02 INFO 139933643601728] #quality_metric: host=algo-1, epoch=6, train rmse <loss>=89.97837475042141\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:02 INFO 139933643601728] #quality_metric: host=algo-1, epoch=6, train mse <loss>=8096.107922727273\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:02 INFO 139933643601728] #quality_metric: host=algo-1, epoch=6, train absolute_loss <loss>=17.719223974609374\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051899.155073, \"EndTime\": 1650051902.1028895, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2947.3726749420166, \"count\": 1, \"min\": 2947.3726749420166, \"max\": 2947.3726749420166}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:02 INFO 139933643601728] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051899.1554804, \"EndTime\": 1650051902.103085, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 6, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1537633.0, \"count\": 1, \"min\": 1537633, \"max\": 1537633}, \"Total Batches Seen\": {\"sum\": 1541.0, \"count\": 1, \"min\": 1541, \"max\": 1541}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 8.0, \"count\": 1, \"min\": 8, \"max\": 8}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:02 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=74471.10139425636 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:02 INFO 139933643601728] #quality_metric: host=algo-1, epoch=7, batch=0 train rmse <loss>=69.21636367218376\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:02 INFO 139933643601728] #quality_metric: host=algo-1, epoch=7, batch=0 train mse <loss>=4790.905\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:02 INFO 139933643601728] #quality_metric: host=algo-1, epoch=7, batch=0 train absolute_loss <loss>=17.67858984375\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:05.217] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 16, \"duration\": 3112, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:05 INFO 139933643601728] #quality_metric: host=algo-1, epoch=7, train rmse <loss>=89.88232149183378\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:05 INFO 139933643601728] #quality_metric: host=algo-1, epoch=7, train mse <loss>=8078.831716761364\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:05 INFO 139933643601728] #quality_metric: host=algo-1, epoch=7, train absolute_loss <loss>=17.620570072798294\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051902.1029444, \"EndTime\": 1650051905.2179692, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 3114.3672466278076, \"count\": 1, \"min\": 3114.3672466278076, \"max\": 3114.3672466278076}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:05 INFO 139933643601728] #progress_metric: host=algo-1, completed 40.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051902.1035779, \"EndTime\": 1650051905.2182167, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 7, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1757152.0, \"count\": 1, \"min\": 1757152, \"max\": 1757152}, \"Total Batches Seen\": {\"sum\": 1761.0, \"count\": 1, \"min\": 1761, \"max\": 1761}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 9.0, \"count\": 1, \"min\": 9, \"max\": 9}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:05 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=70477.24743635434 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:05 INFO 139933643601728] #quality_metric: host=algo-1, epoch=8, batch=0 train rmse <loss>=69.12171511182285\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:05 INFO 139933643601728] #quality_metric: host=algo-1, epoch=8, batch=0 train mse <loss>=4777.8115\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:05 INFO 139933643601728] #quality_metric: host=algo-1, epoch=8, batch=0 train absolute_loss <loss>=17.6106328125\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:07.965] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 18, \"duration\": 2745, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:07 INFO 139933643601728] #quality_metric: host=algo-1, epoch=8, train rmse <loss>=89.79394862019834\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:07 INFO 139933643601728] #quality_metric: host=algo-1, epoch=8, train mse <loss>=8062.953208806818\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:07 INFO 139933643601728] #quality_metric: host=algo-1, epoch=8, train absolute_loss <loss>=17.54735018643466\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051905.2180364, \"EndTime\": 1650051907.965789, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2747.2805976867676, \"count\": 1, \"min\": 2747.2805976867676, \"max\": 2747.2805976867676}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:07 INFO 139933643601728] #progress_metric: host=algo-1, completed 45.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051905.2184813, \"EndTime\": 1650051907.9659934, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 8, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 1976671.0, \"count\": 1, \"min\": 1976671, \"max\": 1976671}, \"Total Batches Seen\": {\"sum\": 1981.0, \"count\": 1, \"min\": 1981, \"max\": 1981}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 10.0, \"count\": 1, \"min\": 10, \"max\": 10}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:07 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=79894.00388723495 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:07 INFO 139933643601728] #quality_metric: host=algo-1, epoch=9, batch=0 train rmse <loss>=69.03420166844838\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:07 INFO 139933643601728] #quality_metric: host=algo-1, epoch=9, batch=0 train mse <loss>=4765.721\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:07 INFO 139933643601728] #quality_metric: host=algo-1, epoch=9, batch=0 train absolute_loss <loss>=17.550154296875\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:10.775] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 20, \"duration\": 2807, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:10 INFO 139933643601728] #quality_metric: host=algo-1, epoch=9, train rmse <loss>=89.71251026638066\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:10 INFO 139933643601728] #quality_metric: host=algo-1, epoch=9, train mse <loss>=8048.334498295455\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:10 INFO 139933643601728] #quality_metric: host=algo-1, epoch=9, train absolute_loss <loss>=17.494896257990057\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051907.9658604, \"EndTime\": 1650051910.7771637, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2810.9116554260254, \"count\": 1, \"min\": 2810.9116554260254, \"max\": 2810.9116554260254}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:10 INFO 139933643601728] #progress_metric: host=algo-1, completed 50.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051907.9662254, \"EndTime\": 1650051910.777388, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 9, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2196190.0, \"count\": 1, \"min\": 2196190, \"max\": 2196190}, \"Total Batches Seen\": {\"sum\": 2201.0, \"count\": 1, \"min\": 2201, \"max\": 2201}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 11.0, \"count\": 1, \"min\": 11, \"max\": 11}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:10 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=78085.79968354508 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:10 INFO 139933643601728] #quality_metric: host=algo-1, epoch=10, batch=0 train rmse <loss>=68.95210294109962\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:10 INFO 139933643601728] #quality_metric: host=algo-1, epoch=10, batch=0 train mse <loss>=4754.3925\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:10 INFO 139933643601728] #quality_metric: host=algo-1, epoch=10, batch=0 train absolute_loss <loss>=17.490455078125\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:13.596] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 22, \"duration\": 2816, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:13 INFO 139933643601728] #quality_metric: host=algo-1, epoch=10, train rmse <loss>=89.63715438616323\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:13 INFO 139933643601728] #quality_metric: host=algo-1, epoch=10, train mse <loss>=8034.819446448863\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:13 INFO 139933643601728] #quality_metric: host=algo-1, epoch=10, train absolute_loss <loss>=17.45398817471591\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051910.7772326, \"EndTime\": 1650051913.597043, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2818.7637329101562, \"count\": 1, \"min\": 2818.7637329101562, \"max\": 2818.7637329101562}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:13 INFO 139933643601728] #progress_metric: host=algo-1, completed 55.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051910.7782536, \"EndTime\": 1650051913.5972533, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 10, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2415709.0, \"count\": 1, \"min\": 2415709, \"max\": 2415709}, \"Total Batches Seen\": {\"sum\": 2421.0, \"count\": 1, \"min\": 2421, \"max\": 2421}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 12.0, \"count\": 1, \"min\": 12, \"max\": 12}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:13 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=77867.93132618035 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:13 INFO 139933643601728] #quality_metric: host=algo-1, epoch=11, batch=0 train rmse <loss>=68.87503901995265\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:13 INFO 139933643601728] #quality_metric: host=algo-1, epoch=11, batch=0 train mse <loss>=4743.771\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:13 INFO 139933643601728] #quality_metric: host=algo-1, epoch=11, batch=0 train absolute_loss <loss>=17.436017578125\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:16.354] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 24, \"duration\": 2753, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:16 INFO 139933643601728] #quality_metric: host=algo-1, epoch=11, train rmse <loss>=89.56761664950523\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:16 INFO 139933643601728] #quality_metric: host=algo-1, epoch=11, train mse <loss>=8022.357952272727\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:16 INFO 139933643601728] #quality_metric: host=algo-1, epoch=11, train absolute_loss <loss>=17.42097587890625\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051913.5971103, \"EndTime\": 1650051916.354993, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2756.376266479492, \"count\": 1, \"min\": 2756.376266479492, \"max\": 2756.376266479492}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:16 INFO 139933643601728] #progress_metric: host=algo-1, completed 60.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051913.5985887, \"EndTime\": 1650051916.3552954, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 11, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2635228.0, \"count\": 1, \"min\": 2635228, \"max\": 2635228}, \"Total Batches Seen\": {\"sum\": 2641.0, \"count\": 1, \"min\": 2641, \"max\": 2641}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 13.0, \"count\": 1, \"min\": 13, \"max\": 13}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:16 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=79627.40888790606 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:16 INFO 139933643601728] #quality_metric: host=algo-1, epoch=12, batch=0 train rmse <loss>=68.80243091635644\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:16 INFO 139933643601728] #quality_metric: host=algo-1, epoch=12, batch=0 train mse <loss>=4733.7745\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:16 INFO 139933643601728] #quality_metric: host=algo-1, epoch=12, batch=0 train absolute_loss <loss>=17.39027734375\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:19.184] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 26, \"duration\": 2827, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:19 INFO 139933643601728] #quality_metric: host=algo-1, epoch=12, train rmse <loss>=89.502190183175\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:19 INFO 139933643601728] #quality_metric: host=algo-1, epoch=12, train mse <loss>=8010.642047585227\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:19 INFO 139933643601728] #quality_metric: host=algo-1, epoch=12, train absolute_loss <loss>=17.38904547674006\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051916.3550828, \"EndTime\": 1650051919.184985, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2829.4336795806885, \"count\": 1, \"min\": 2829.4336795806885, \"max\": 2829.4336795806885}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:19 INFO 139933643601728] #progress_metric: host=algo-1, completed 65.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051916.3555262, \"EndTime\": 1650051919.1851323, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 12, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 2854747.0, \"count\": 1, \"min\": 2854747, \"max\": 2854747}, \"Total Batches Seen\": {\"sum\": 2861.0, \"count\": 1, \"min\": 2861, \"max\": 2861}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 14.0, \"count\": 1, \"min\": 14, \"max\": 14}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:19 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=77577.08885465095 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:19 INFO 139933643601728] #quality_metric: host=algo-1, epoch=13, batch=0 train rmse <loss>=68.73474376179779\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:19 INFO 139933643601728] #quality_metric: host=algo-1, epoch=13, batch=0 train mse <loss>=4724.465\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:19 INFO 139933643601728] #quality_metric: host=algo-1, epoch=13, batch=0 train absolute_loss <loss>=17.34413671875\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:21.947] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 28, \"duration\": 2760, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:21 INFO 139933643601728] #quality_metric: host=algo-1, epoch=13, train rmse <loss>=89.44082671374765\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:21 INFO 139933643601728] #quality_metric: host=algo-1, epoch=13, train mse <loss>=7999.661483238637\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:21 INFO 139933643601728] #quality_metric: host=algo-1, epoch=13, train absolute_loss <loss>=17.362610808771308\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051919.1850379, \"EndTime\": 1650051921.9474938, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2762.1841430664062, \"count\": 1, \"min\": 2762.1841430664062, \"max\": 2762.1841430664062}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:21 INFO 139933643601728] #progress_metric: host=algo-1, completed 70.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051919.1852894, \"EndTime\": 1650051921.9476385, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 13, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3074266.0, \"count\": 1, \"min\": 3074266, \"max\": 3074266}, \"Total Batches Seen\": {\"sum\": 3081.0, \"count\": 1, \"min\": 3081, \"max\": 3081}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 15.0, \"count\": 1, \"min\": 15, \"max\": 15}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:21 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=79466.00925908664 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:21 INFO 139933643601728] #quality_metric: host=algo-1, epoch=14, batch=0 train rmse <loss>=68.67233795350207\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:21 INFO 139933643601728] #quality_metric: host=algo-1, epoch=14, batch=0 train mse <loss>=4715.89\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:21 INFO 139933643601728] #quality_metric: host=algo-1, epoch=14, batch=0 train absolute_loss <loss>=17.292935546875\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:24.756] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 30, \"duration\": 2806, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:24 INFO 139933643601728] #quality_metric: host=algo-1, epoch=14, train rmse <loss>=89.38316585868087\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:24 INFO 139933643601728] #quality_metric: host=algo-1, epoch=14, train mse <loss>=7989.350338920454\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:24 INFO 139933643601728] #quality_metric: host=algo-1, epoch=14, train absolute_loss <loss>=17.333876198508523\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051921.9475465, \"EndTime\": 1650051924.7566721, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2808.347702026367, \"count\": 1, \"min\": 2808.347702026367, \"max\": 2808.347702026367}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:24 INFO 139933643601728] #progress_metric: host=algo-1, completed 75.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051921.9483018, \"EndTime\": 1650051924.7568164, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 14, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3293785.0, \"count\": 1, \"min\": 3293785, \"max\": 3293785}, \"Total Batches Seen\": {\"sum\": 3301.0, \"count\": 1, \"min\": 3301, \"max\": 3301}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 16.0, \"count\": 1, \"min\": 16, \"max\": 16}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:24 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=78159.60924805677 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:24 INFO 139933643601728] #quality_metric: host=algo-1, epoch=15, batch=0 train rmse <loss>=68.61341705526696\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:24 INFO 139933643601728] #quality_metric: host=algo-1, epoch=15, batch=0 train mse <loss>=4707.801\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:24 INFO 139933643601728] #quality_metric: host=algo-1, epoch=15, batch=0 train absolute_loss <loss>=17.2486875\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:27.536] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 32, \"duration\": 2778, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:27 INFO 139933643601728] #quality_metric: host=algo-1, epoch=15, train rmse <loss>=89.32885107497465\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:27 INFO 139933643601728] #quality_metric: host=algo-1, epoch=15, train mse <loss>=7979.643634375\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:27 INFO 139933643601728] #quality_metric: host=algo-1, epoch=15, train absolute_loss <loss>=17.30803719815341\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051924.7567234, \"EndTime\": 1650051927.537201, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2780.2090644836426, \"count\": 1, \"min\": 2780.2090644836426, \"max\": 2780.2090644836426}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:27 INFO 139933643601728] #progress_metric: host=algo-1, completed 80.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051924.756968, \"EndTime\": 1650051927.5373998, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 15, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3513304.0, \"count\": 1, \"min\": 3513304, \"max\": 3513304}, \"Total Batches Seen\": {\"sum\": 3521.0, \"count\": 1, \"min\": 3521, \"max\": 3521}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 17.0, \"count\": 1, \"min\": 17, \"max\": 17}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:27 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=78948.46269574962 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:27 INFO 139933643601728] #quality_metric: host=algo-1, epoch=16, batch=0 train rmse <loss>=68.55855161830652\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:27 INFO 139933643601728] #quality_metric: host=algo-1, epoch=16, batch=0 train mse <loss>=4700.275\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:27 INFO 139933643601728] #quality_metric: host=algo-1, epoch=16, batch=0 train absolute_loss <loss>=17.207400390625\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:30.245] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 34, \"duration\": 2706, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:30 INFO 139933643601728] #quality_metric: host=algo-1, epoch=16, train rmse <loss>=89.2773943729471\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:30 INFO 139933643601728] #quality_metric: host=algo-1, epoch=16, train mse <loss>=7970.453146022727\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:30 INFO 139933643601728] #quality_metric: host=algo-1, epoch=16, train absolute_loss <loss>=17.283455996981534\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051927.5372658, \"EndTime\": 1650051930.2460093, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2708.355188369751, \"count\": 1, \"min\": 2708.355188369751, \"max\": 2708.355188369751}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:30 INFO 139933643601728] #progress_metric: host=algo-1, completed 85.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051927.5376291, \"EndTime\": 1650051930.2462275, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 16, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3732823.0, \"count\": 1, \"min\": 3732823, \"max\": 3732823}, \"Total Batches Seen\": {\"sum\": 3741.0, \"count\": 1, \"min\": 3741, \"max\": 3741}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 18.0, \"count\": 1, \"min\": 18, \"max\": 18}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:30 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=81041.95657884835 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:30 INFO 139933643601728] #quality_metric: host=algo-1, epoch=17, batch=0 train rmse <loss>=68.50716750822501\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:30 INFO 139933643601728] #quality_metric: host=algo-1, epoch=17, batch=0 train mse <loss>=4693.232\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:30 INFO 139933643601728] #quality_metric: host=algo-1, epoch=17, batch=0 train absolute_loss <loss>=17.1710078125\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:33.120] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 36, \"duration\": 2871, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:33 INFO 139933643601728] #quality_metric: host=algo-1, epoch=17, train rmse <loss>=89.22836755916096\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:33 INFO 139933643601728] #quality_metric: host=algo-1, epoch=17, train mse <loss>=7961.701577272727\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:33 INFO 139933643601728] #quality_metric: host=algo-1, epoch=17, train absolute_loss <loss>=17.25918828568892\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051930.2460766, \"EndTime\": 1650051933.1208594, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2873.628616333008, \"count\": 1, \"min\": 2873.628616333008, \"max\": 2873.628616333008}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:33 INFO 139933643601728] #progress_metric: host=algo-1, completed 90.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051930.2472048, \"EndTime\": 1650051933.1210766, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 17, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 3952342.0, \"count\": 1, \"min\": 3952342, \"max\": 3952342}, \"Total Batches Seen\": {\"sum\": 3961.0, \"count\": 1, \"min\": 3961, \"max\": 3961}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 19.0, \"count\": 1, \"min\": 19, \"max\": 19}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:33 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=76381.72884780263 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:33 INFO 139933643601728] #quality_metric: host=algo-1, epoch=18, batch=0 train rmse <loss>=68.45944785053412\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:33 INFO 139933643601728] #quality_metric: host=algo-1, epoch=18, batch=0 train mse <loss>=4686.696\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:33 INFO 139933643601728] #quality_metric: host=algo-1, epoch=18, batch=0 train absolute_loss <loss>=17.1378046875\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:35.861] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 38, \"duration\": 2738, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:35 INFO 139933643601728] #quality_metric: host=algo-1, epoch=18, train rmse <loss>=89.18173190559723\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:35 INFO 139933643601728] #quality_metric: host=algo-1, epoch=18, train mse <loss>=7953.381305681818\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:35 INFO 139933643601728] #quality_metric: host=algo-1, epoch=18, train absolute_loss <loss>=17.23677677112926\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051933.1209252, \"EndTime\": 1650051935.8623023, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2740.9980297088623, \"count\": 1, \"min\": 2740.9980297088623, \"max\": 2740.9980297088623}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:35 INFO 139933643601728] #progress_metric: host=algo-1, completed 95.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051933.1212804, \"EndTime\": 1650051935.8625095, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 18, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4171861.0, \"count\": 1, \"min\": 4171861, \"max\": 4171861}, \"Total Batches Seen\": {\"sum\": 4181.0, \"count\": 1, \"min\": 4181, \"max\": 4181}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 20.0, \"count\": 1, \"min\": 20, \"max\": 20}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:35 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=80077.89056997512 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:35 INFO 139933643601728] #quality_metric: host=algo-1, epoch=19, batch=0 train rmse <loss>=68.41493988888685\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:35 INFO 139933643601728] #quality_metric: host=algo-1, epoch=19, batch=0 train mse <loss>=4680.604\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:35 INFO 139933643601728] #quality_metric: host=algo-1, epoch=19, batch=0 train absolute_loss <loss>=17.10520703125\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:38.664] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 40, \"duration\": 2800, \"num_examples\": 220, \"num_bytes\": 21856132}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #quality_metric: host=algo-1, epoch=19, train rmse <loss>=89.13712189336056\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #quality_metric: host=algo-1, epoch=19, train mse <loss>=7945.426499431818\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #quality_metric: host=algo-1, epoch=19, train absolute_loss <loss>=17.212976438210227\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #quality_metric: host=algo-1, train rmse <loss>=89.13712189336056\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #quality_metric: host=algo-1, train mse <loss>=7945.426499431818\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #quality_metric: host=algo-1, train absolute_loss <loss>=17.212976438210227\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051935.8623667, \"EndTime\": 1650051938.6655638, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 2802.842855453491, \"count\": 1, \"min\": 2802.842855453491, \"max\": 2802.842855453491}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051935.8626935, \"EndTime\": 1650051938.6657743, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 19, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 4391380.0, \"count\": 1, \"min\": 4391380, \"max\": 4391380}, \"Total Batches Seen\": {\"sum\": 4401.0, \"count\": 1, \"min\": 4401, \"max\": 4401}, \"Max Records Seen Between Resets\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Max Batches Seen Between Resets\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}, \"Reset Count\": {\"sum\": 21.0, \"count\": 1, \"min\": 21, \"max\": 21}, \"Number of Records Since Last Reset\": {\"sum\": 219519.0, \"count\": 1, \"min\": 219519, \"max\": 219519}, \"Number of Batches Since Last Reset\": {\"sum\": 220.0, \"count\": 1, \"min\": 220, \"max\": 220}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #throughput_metric: host=algo-1, train throughput=78310.2652551934 records/second\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 WARNING 139933643601728] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] Pulling entire model from kvstore to finalize\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051938.665625, \"EndTime\": 1650051938.6711044, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 5.085468292236328, \"count\": 1, \"min\": 5.085468292236328, \"max\": 5.085468292236328}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] Saved checkpoint to \"/tmp/tmpvk0y5m0b/state-0001.params\"\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:38.688] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 0, \"duration\": 56556, \"num_examples\": 1, \"num_bytes\": 99820}\u001b[0m\n",
      "\u001b[34m[2022-04-15 19:45:38.922] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/test\", \"epoch\": 1, \"duration\": 233, \"num_examples\": 55, \"num_bytes\": 5462876}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051938.6882167, \"EndTime\": 1650051938.922684, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"test_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Total Batches Seen\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}, \"Max Records Seen Between Resets\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Max Batches Seen Between Resets\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 54880.0, \"count\": 1, \"min\": 54880, \"max\": 54880}, \"Number of Batches Since Last Reset\": {\"sum\": 55.0, \"count\": 1, \"min\": 55, \"max\": 55}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #test_score (algo-1) : ('rmse', 68.74431145296938)\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #test_score (algo-1) : ('mse', 4725.780357142857)\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #test_score (algo-1) : ('absolute_loss', 17.043618124914587)\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #quality_metric: host=algo-1, test rmse <loss>=68.74431145296938\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #quality_metric: host=algo-1, test mse <loss>=4725.780357142857\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728] #quality_metric: host=algo-1, test absolute_loss <loss>=17.043618124914587\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1650051938.6711783, \"EndTime\": 1650051938.9238377, \"Dimensions\": {\"Algorithm\": \"factorization-machines\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 20.181894302368164, \"count\": 1, \"min\": 20.181894302368164, \"max\": 20.181894302368164}, \"totaltime\": {\"sum\": 56831.74633979797, \"count\": 1, \"min\": 56831.74633979797, \"max\": 56831.74633979797}}}\u001b[0m\n",
      "\u001b[34m[04/15/2022 19:45:38 INFO 139933643601728 integration.py:636] worker closed\u001b[0m\n",
      "\n",
      "2022-04-15 19:45:58 Uploading - Uploading generated training model\n",
      "2022-04-15 19:45:58 Completed - Training job completed\n",
      "Training seconds: 153\n",
      "Billable seconds: 153\n",
      "Stored 'training_job_name' (str)\n"
     ]
    }
   ],
   "source": [
    "if 'training_job_name' not in locals():\n",
    "    \n",
    "    fm.fit({'train': train_data_location, 'test': test_data_location})\n",
    "    training_job_name = fm.latest_training_job.job_name\n",
    "    %store training_job_name\n",
    "    \n",
    "else:\n",
    "    print(f'Using previous training job: {training_job_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once our model is trained, we will want to use SageMaker ML Lineage Tracking to track different artifacts about the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_job_info = sagemaker_boto_client.describe_training_job(TrainingJobName=training_job_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training data artifact\n",
    "\n",
    "And important aspect of transparency in machine learning is to be able to link a model with code and data used to train it, so we can easily find how any given model was produced. In the event that a model begins to start perfroming poorly, we should be able to quickly find all of all artifacts used to create it, so we can debug to the problem at the source.\n",
    "\n",
    "To do this, we need to create an artifact for every aspect of training we'd like to save.\n",
    "\n",
    "First, we will create an artifact containing training data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:us-east-1:267710284436:artifact/354fcb101b03a6f2b0651cd633f7fef5\n"
     ]
    }
   ],
   "source": [
    "training_data_s3_uri = training_job_info[\"InputDataConfig\"][0][\"DataSource\"][\"S3DataSource\"][\n",
    "    \"S3Uri\"\n",
    "]\n",
    "\n",
    "matching_artifacts = list(\n",
    "    artifact.Artifact.list(source_uri=training_data_s3_uri, sagemaker_session=sagemaker_session)\n",
    ")\n",
    "\n",
    "if matching_artifacts:\n",
    "    training_data_artifact = matching_artifacts[0]\n",
    "    print(f\"Using existing artifact: {training_data_artifact.artifact_arn}\")\n",
    "else:\n",
    "    training_data_artifact = artifact.Artifact.create(\n",
    "        artifact_name=\"TrainingData\",\n",
    "        source_uri=training_data_s3_uri,\n",
    "        artifact_type=\"Dataset\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    print(f\"Create artifact {training_data_artifact.artifact_arn}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model artifact\n",
    "\n",
    "Then we will create an artifact containing the model itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using existing artifact: arn:aws:sagemaker:us-east-1:267710284436:artifact/cdb4ea6675f5b2b676783bbddccfb2fa\n"
     ]
    }
   ],
   "source": [
    "trained_model_s3_uri = training_job_info[\"ModelArtifacts\"][\"S3ModelArtifacts\"]\n",
    "\n",
    "matching_artifacts = list(\n",
    "    artifact.Artifact.list(source_uri=trained_model_s3_uri, sagemaker_session=sagemaker_session)\n",
    ")\n",
    "\n",
    "if matching_artifacts:\n",
    "    model_artifact = matching_artifacts[0]\n",
    "    print(f\"Using existing artifact: {model_artifact.artifact_arn}\")\n",
    "else:\n",
    "    model_artifact = artifact.Artifact.create(\n",
    "        artifact_name=\"TrainedModel\",\n",
    "        source_uri=trained_model_s3_uri,\n",
    "        artifact_type=\"Model\",\n",
    "        sagemaker_session=sagemaker_session,\n",
    "    )\n",
    "    print(f\"Create artifact {model_artifact.artifact_arn}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set artifact associations\n",
    "\n",
    "We can then set the associations for these artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_component = sagemaker_boto_client.describe_trial_component(\n",
    "    TrialComponentName=training_job_name + \"-aws-training-job\"\n",
    ")\n",
    "trial_component_arn = trial_component[\"TrialComponentArn\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Association already exists with DataSet\n",
      "Association with Model: SUCCEESFUL\n"
     ]
    }
   ],
   "source": [
    "artifact_list = [[training_data_artifact, \"ContributedTo\"], [model_artifact, \"Produced\"]]\n",
    "\n",
    "for art, assoc in artifact_list:\n",
    "    try:\n",
    "        association.Association.create(\n",
    "            source_arn=art.artifact_arn,\n",
    "            destination_arn=trial_component_arn,\n",
    "            association_type=assoc,\n",
    "            sagemaker_session=sagemaker_session,\n",
    "        )\n",
    "        print(f\"Association with {art.artifact_type}: SUCCEESFUL\")\n",
    "    except:\n",
    "        print(f\"Association already exists with {art.artifact_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model retail-recommendations\n"
     ]
    }
   ],
   "source": [
    "model_name = \"retail-recommendations\"\n",
    "model_matches = sagemaker_boto_client.list_models(NameContains=model_name)[\"Models\"]\n",
    "\n",
    "if not model_matches:\n",
    "    print(f\"Creating model {model_name}\")\n",
    "    model = sagemaker_session.create_model_from_job(\n",
    "        name=model_name,\n",
    "        training_job_name=training_job_info[\"TrainingJobName\"],\n",
    "        role=sagemaker_role,\n",
    "        image_uri=training_job_info[\"AlgorithmSpecification\"][\"TrainingImage\"],\n",
    "    )\n",
    "else:\n",
    "    print(f\"Model {model_name} already exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SageMaker Model Registry\n",
    "\n",
    "Once a useful model has been trained and its artifacts properly associated, the next step is to register the model for future reference and possible deployment. This allows you to gain visibility and reproducibility of the process by which we create the models. \n",
    "\n",
    "### Create Model Package Group\n",
    "\n",
    "A Model Package Groups holds multiple versions or iterations of a model. Though it is not required to create them for every model in the registry, they help organize various models which all have the same purpose and provide autiomatic versioning.\n",
    "\n",
    "\n",
    "We create model package group that contains different version of a model. You can register each model you train, and the model registry adds it to the model group as a new model version. If you plan on tracking multiple version of a model, you need to create a model package group first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'mpg_name' (str)\n",
      "Model Package Group name: retail-recommendation-2022-04-15-20-10\n"
     ]
    }
   ],
   "source": [
    "if 'mpg_name' not in locals():\n",
    "    timestamp = datetime.datetime.now().strftime('%Y-%m-%d-%H-%M')\n",
    "    mpg_name = f'retail-recommendation-{timestamp}'\n",
    "    %store mpg_name\n",
    "\n",
    "print(f'Model Package Group name: {mpg_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpg_input_dict = {\n",
    "    \"ModelPackageGroupName\": mpg_name,\n",
    "    \"ModelPackageGroupDescription\": \"Recommendation for Online Retail Sales\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create Model Package Group retail-recommendation-2022-04-15-20-10: SUCCESSFUL\n"
     ]
    }
   ],
   "source": [
    "matching_mpg = sagemaker_boto_client.list_model_package_groups(NameContains=mpg_name)[\n",
    "    \"ModelPackageGroupSummaryList\"\n",
    "]\n",
    "\n",
    "if matching_mpg:\n",
    "    print(f\"Using existing Model Package Group: {mpg_name}\")\n",
    "else:\n",
    "    mpg_response = sagemaker_boto_client.create_model_package_group(**mpg_input_dict)\n",
    "    print(f\"Create Model Package Group {mpg_name}: SUCCESSFUL\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model package group has been created, you can now add the first version of your trained model to it.\n",
    "We will only show one version of a model here, but in practice, you may have different versions as you try different features, different hyperparameters, or even update your model over time. \n",
    "\n",
    "Along with s3 location of the trained model, you need to add the inference specification, so we know what Docker image to use to deploy the model and make the prediction. \n",
    "We can also use various training metrics like training and validation, group mean square error. \n",
    "\n",
    "Here we inlcude every metric that is associated with our training job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics_report = {\"regression_metrics\": {}}\n",
    "\n",
    "for metric in training_job_info[\"FinalMetricDataList\"]:\n",
    "    stat = {metric[\"MetricName\"]: {\"value\": metric[\"Value\"]}}\n",
    "    model_metrics_report[\"regression_metrics\"].update(stat)\n",
    "\n",
    "with open(\"training_metrics.json\", \"w\") as f:\n",
    "    json.dump(model_metrics_report, f)\n",
    "\n",
    "metrics_s3_key = f\"training_jobs/{training_job_info['TrainingJobName']}/training_metrics.json\"\n",
    "s3_client.upload_file(Filename=\"training_metrics.json\", Bucket=bucket, Key=metrics_s3_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the inference spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_inference_spec = InferenceSpecification().get_inference_specification_dict(\n",
    "    ecr_image=training_job_info[\"AlgorithmSpecification\"][\"TrainingImage\"],\n",
    "    supports_gpu=False,\n",
    "    supported_content_types=[\"application/x-recordio-protobuf\", \"application/json\"],\n",
    "    supported_mime_types=[\"text/csv\"],\n",
    ")\n",
    "\n",
    "mp_inference_spec[\"InferenceSpecification\"][\"Containers\"][0][\"ModelDataUrl\"] = training_job_info[\n",
    "    \"ModelArtifacts\"\n",
    "][\"S3ModelArtifacts\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define model metrics\n",
    "\n",
    "Metrics other than model quality can be defined. See the Boto3 documentation for [creating a model package](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/sagemaker.html#SageMaker.Client.create_model_package)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {\n",
    "    \"ModelQuality\": {\n",
    "        \"Statistics\": {\n",
    "            \"ContentType\": \"application/json\",\n",
    "            \"S3Uri\": f\"s3://{bucket}/{metrics_s3_key}\",\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, for model governance reasons, you can tag the model package with a status, it's easy to tell which models are actually approved or declined for deployment into production. \n",
    "Here we tage \"PendingManualApproval\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp_input_dict = {\n",
    "    \"ModelPackageGroupName\": mpg_name,\n",
    "    \"ModelPackageDescription\": \"Factorization Machine Model to create personalized retail recommendations\",\n",
    "    \"ModelApprovalStatus\": \"PendingManualApproval\",\n",
    "    \"ModelMetrics\": model_metrics,\n",
    "}\n",
    "\n",
    "mp_input_dict.update(mp_inference_spec)\n",
    "mp_response = sagemaker_boto_client.create_model_package(**mp_input_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait until model package is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model package status: Completed\n"
     ]
    }
   ],
   "source": [
    "mp_info = sagemaker_boto_client.describe_model_package(\n",
    "    ModelPackageName=mp_response[\"ModelPackageArn\"]\n",
    ")\n",
    "mp_status = mp_info[\"ModelPackageStatus\"]\n",
    "\n",
    "while mp_status not in [\"Completed\", \"Failed\"]:\n",
    "    time.sleep(5)\n",
    "    mp_info = sagemaker_boto_client.describe_model_package(\n",
    "        ModelPackageName=mp_response[\"ModelPackageArn\"]\n",
    "    )\n",
    "    mp_status = mp_info[\"ModelPackageStatus\"]\n",
    "    print(f\"model package status: {mp_status}\")\n",
    "print(f\"model package status: {mp_status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_package = sagemaker_boto_client.list_model_packages(ModelPackageGroupName=mpg_name)[\n",
    "    \"ModelPackageSummaryList\"\n",
    "][0]\n",
    "model_package_update = {\n",
    "    \"ModelPackageArn\": model_package[\"ModelPackageArn\"],\n",
    "    \"ModelApprovalStatus\": \"Approved\",\n",
    "}\n",
    "\n",
    "update_response = sagemaker_boto_client.update_model_package(**model_package_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create endpoint config and endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'endpoint_config_name' (str)\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: Creating\n",
      "Endpoint status: InService\n"
     ]
    }
   ],
   "source": [
    "primary_container = {'ModelPackageName' : model_package['ModelPackageArn']}\n",
    "endpoint_config_name = f'{model_name}-endpoint-config'\n",
    "existing_configs = sagemaker_boto_client.list_endpoint_configs(NameContains = endpoint_config_name)['EndpointConfigs']\n",
    "\n",
    "if not existing_configs:\n",
    "    create_ep_config_response = sagemaker_boto_client.create_endpoint_config(\n",
    "        EndpointConfigName=endpoint_config_name,\n",
    "        ProductionVariants=[\n",
    "            {\n",
    "                \"InstanceType\": 'ml.m4.4xlarge',\n",
    "                \"InitialVariantWeight\": 1,\n",
    "                \"InitialInstanceCount\": 1,\n",
    "                \"ModelName\": model_name,\n",
    "                \"VariantName\": \"AllTraffic\",\n",
    "            }]\n",
    "    )\n",
    "    %store endpoint_config_name\n",
    "    \n",
    "\n",
    "endpoint_name = f'{model_name}-endpoint'\n",
    "existing_endpoints = sagemaker_boto_client.list_endpoints(NameContains=endpoint_name)[\"Endpoints\"]\n",
    "\n",
    "if not existing_endpoints:\n",
    "    create_endpoint_response = sagemaker_boto_client.create_endpoint(\n",
    "        EndpointName=endpoint_name, \n",
    "        EndpointConfigName=endpoint_config_name)\n",
    "     \n",
    "\n",
    "endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "endpoint_status = endpoint_info[\"EndpointStatus\"]\n",
    "\n",
    "while endpoint_status == \"Creating\":\n",
    "    endpoint_info = sagemaker_boto_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    endpoint_status = endpoint_info[\"EndpointStatus\"]\n",
    "    print(\"Endpoint status:\", endpoint_status)\n",
    "    if endpoint_status == \"Creating\":\n",
    "        time.sleep(60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Predictions\n",
    "\n",
    "Now that we've trained our model, we can deploy it behind an Amazon SageMaker real-time hosted endpoint. This will allow out to make predictions (or inference) from the model dyanamically.\n",
    "\n",
    "Note, Amazon SageMaker allows you the flexibility of importing models trained elsewhere, as well as the choice of not importing models if the target of model creation is AWS Lambda, AWS Greengrass, Amazon Redshift, Amazon Athena, or other deployment target.\n",
    "\n",
    "Here we will take the top customer, the customer who spent the most money, and try to find which items to recommend to them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.deserializers import JSONDeserializer\n",
    "from sagemaker.serializers import JSONSerializer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!"
     ]
    }
   ],
   "source": [
    "class FMSerializer(JSONSerializer):\n",
    "    def serialize(self, data):\n",
    "        js = {\"instances\": []}\n",
    "        for row in data:\n",
    "            js[\"instances\"].append({\"features\": row.tolist()})\n",
    "        return json.dumps(js)\n",
    "\n",
    "\n",
    "fm_predictor = fm.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    serializer=FMSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find customer who spent the most money\n",
    "\n",
    "df[\"invoice_amount\"] = df[\"Quantity\"] * df[\"UnitPrice\"]\n",
    "top_customer = (\n",
    "    df.groupby(\"CustomerID\").sum()[\"invoice_amount\"].sort_values(ascending=False).index[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(df, customer_id, n_recommendations, n_ranks=100):\n",
    "    popular_items = (\n",
    "        df.groupby([\"StockCode\", \"UnitPrice\"])\n",
    "        .nunique()[\"CustomerID\"]\n",
    "        .sort_values(ascending=False)\n",
    "        .reset_index()\n",
    "    )\n",
    "    top_n_items = popular_items[\"StockCode\"].iloc[:n_ranks].values\n",
    "    top_n_prices = popular_items[\"UnitPrice\"].iloc[:n_ranks].values\n",
    "\n",
    "    # stock codes can have multiple descriptions, so we will choose whichever description is most common\n",
    "    item_map = df.groupby(\"StockCode\").agg(lambda x: x.value_counts().index[0])[\"Description\"]\n",
    "\n",
    "    # find customer's country\n",
    "    df_subset = df.loc[df[\"CustomerID\"] == customer_id]\n",
    "    country = df_subset[\"Country\"].value_counts().index[0]\n",
    "\n",
    "    data = {\n",
    "        \"StockCode\": top_n_items,\n",
    "        \"Description\": [item_map[i] for i in top_n_items],\n",
    "        \"CustomerID\": customer_id,\n",
    "        \"Country\": country,\n",
    "        \"UnitPrice\": top_n_prices,\n",
    "    }\n",
    "\n",
    "    df_inference = pd.DataFrame(data)\n",
    "\n",
    "    # we need to build the data set similar to how we built it for training\n",
    "    # it should have the same number of features as the training data\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\")\n",
    "    onehot_cols = [\"StockCode\", \"CustomerID\", \"Country\"]\n",
    "    enc.fit(df[onehot_cols])\n",
    "    onehot_output = enc.transform(df_inference[onehot_cols])\n",
    "\n",
    "    vectorizer = TfidfVectorizer(min_df=2)\n",
    "    unique_descriptions = df[\"Description\"].unique()\n",
    "    vectorizer.fit(unique_descriptions)\n",
    "    tfidf_output = vectorizer.transform(df_inference[\"Description\"])\n",
    "\n",
    "    row = range(len(df_inference))\n",
    "    col = [0] * len(df_inference)\n",
    "    unit_price = csr_matrix((df_inference[\"UnitPrice\"].values, (row, col)), dtype=\"float32\")\n",
    "\n",
    "    X_inference = hstack([onehot_output, tfidf_output, unit_price], format=\"csr\")\n",
    "\n",
    "    result = fm_predictor.predict(X_inference.toarray())\n",
    "    preds = [i[\"score\"] for i in result[\"predictions\"]]\n",
    "    index_array = np.array(preds).argsort()\n",
    "    items = enc.inverse_transform(onehot_output)[:, 0]\n",
    "    top_recs = np.take_along_axis(items, index_array, axis=0)[: -n_recommendations - 1 : -1]\n",
    "    recommendations = [[i, item_map[i]] for i in top_recs]\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 recommended products:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[['22423', 'REGENCY CAKESTAND 3 TIER'],\n",
       " ['22776', 'SWEETHEART CAKESTAND 3 TIER'],\n",
       " ['22624', 'IVORY KITCHEN SCALES'],\n",
       " ['85123A', 'WHITE HANGING HEART T-LIGHT HOLDER'],\n",
       " ['85099B', 'JUMBO BAG RED RETROSPOT']]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Top 5 recommended products:\")\n",
    "get_recommendations(df, top_customer, n_recommendations=5, n_ranks=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Pipeline\n",
    "\n",
    "Now that we are comfotable with the model that we built.\n",
    "Model training is not one-and-done task. You will want to improve your model with new data as well as changes in customer purchase behavior over time. Improving models takes thousands of repetitions of all the steps we just went through. \n",
    "Having these steps automated using Pipielines allows to reply the whole workflow with single click and save hours of manual work.\n",
    "\n",
    "First let's upload our raw data to s3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_uri = f\"s3://{bucket}/data\"\n",
    "input_data_uri = sagemaker.s3.S3Uploader.upload(\n",
    "    local_path=\"data/Online Retail.csv\", desired_s3_uri=base_uri\n",
    ")\n",
    "\n",
    "input_data = ParameterString(name=\"InputData\", default_value=input_data_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will then set a model approvel status of Approved, meaning this model is approved for production. \n",
    "In pracitice, you might want to set this to Pending manual approval, so that someone needs to go in and manually approve the model for production later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\", default_value=\"PendingManualApproval\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define a customeized step for preprocessing  our data. Our preprocessing.py file has all the transmation that we perfromed above, including spiltting the data into training and testing sets and saving them to protobuf format. \n",
    "We specify the outputs to contain both the training data and testing data, so we can use them in subsequent steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_dataset_script_uri = f\"s3://{bucket}/{prefix}/code/preprocessing.py\"\n",
    "s3_client.upload_file(\n",
    "    Filename=\"preprocessing.py\", Bucket=bucket, Key=f\"{prefix}/code/preprocessing.py\"\n",
    ")\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    instance_count=1,\n",
    "    base_job_name=\"sklearn-retail-sales-process\",\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "\n",
    "create_dataset_step = ProcessingStep(\n",
    "    name=\"PreprocessData\",\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        sagemaker.processing.ProcessingInput(\n",
    "            source=input_data, destination=\"/opt/ml/processing/input\"\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"train_data\", source=\"/opt/ml/processing/output/train\"\n",
    "        ),\n",
    "        sagemaker.processing.ProcessingOutput(\n",
    "            output_name=\"test_data\", source=\"/opt/ml/processing/output/test\"\n",
    "        ),\n",
    "    ],\n",
    "    code=create_dataset_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SageMaker comes with a pre-made training step in the SageMaker SDK, which abstracts away most of the generic processor code. All we need to give this step is the estimator and an input, which, in this case, is the training and testing data. \n",
    "These are the same training and testing data that we created above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = TrainingStep(\n",
    "    name=\"TrainingStep\",\n",
    "    estimator=fm,\n",
    "    inputs={\n",
    "        \"train\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=create_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train_data\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "        \"test\": sagemaker.inputs.TrainingInput(\n",
    "            s3_data=create_dataset_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"test_data\"\n",
    "            ].S3Output.S3Uri\n",
    "        ),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sagemaker.model.Model(\n",
    "    name=\"retail-personalization-factorization-machine\",\n",
    "    image_uri=train_step.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=sagemaker_role,\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(instance_type=\"ml.m4.xlarge\")\n",
    "\n",
    "create_model_step = CreateModelStep(name=\"CreateModel\", model=model, inputs=inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "register_step = RegisterModel(\n",
    "    name=\"RegisterModel\",\n",
    "    estimator=fm,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/x-recordio-protobuf\", \"application/json\"],\n",
    "    response_types=[\"text/csv\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=mpg_name,\n",
    "    approval_status=model_approval_status,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.upload_file(Filename=\"deploy.py\", Bucket=bucket, Key=f\"{prefix}/code/deploy.py\")\n",
    "deploy_script_uri = f\"s3://{bucket}/{prefix}/code/deploy.py\"\n",
    "\n",
    "deployment_processor = SKLearnProcessor(\n",
    "    framework_version=\"0.23-1\",\n",
    "    role=sagemaker_role,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    instance_count=1,\n",
    "    base_job_name=f\"{prefix}-deploy\",\n",
    "    sagemaker_session=sagemaker_session,\n",
    ")\n",
    "\n",
    "deploy_step = ProcessingStep(\n",
    "    name=\"DeployModel\",\n",
    "    processor=deployment_processor,\n",
    "    job_arguments=[\n",
    "        \"--model-name\",\n",
    "        create_model_step.properties.ModelName,\n",
    "        \"--region\",\n",
    "        region,\n",
    "        \"--endpoint-instance-type\",\n",
    "        \"ml.m4.xlarge\",\n",
    "        \"--endpoint-name\",\n",
    "        \"retail-recommendation-endpoint\",\n",
    "    ],\n",
    "    code=deploy_script_uri,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'pipeline_name' (str)\n"
     ]
    }
   ],
   "source": [
    "pipeline_name = f\"PersonalizationDemo\"\n",
    "%store pipeline_name\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[input_data, model_approval_status],\n",
    "    steps=[create_dataset_step, train_step, create_model_step, register_step, deploy_step],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-489ba6965c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrole_arn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_role\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/workflow/pipeline.py\u001b[0m in \u001b[0;36mupsert\u001b[0;34m(self, role_arn, description, tags, parallelism_config)\u001b[0m\n\u001b[1;32m    217\u001b[0m         \"\"\"\n\u001b[1;32m    218\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrole_arn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallelism_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClientError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/workflow/pipeline.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, role_arn, description, tags, parallelism_config)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \"\"\"\n\u001b[1;32m    115\u001b[0m         \u001b[0mtags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_append_project_tags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrole_arn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparallelism_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         update_args(\n\u001b[1;32m    118\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/workflow/pipeline.py\u001b[0m in \u001b[0;36m_create_args\u001b[0;34m(self, role_arn, description, parallelism_config)\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mA\u001b[0m \u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcalling\u001b[0m \u001b[0mcreate_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \"\"\"\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mpipeline_definition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         kwargs = dict(\n\u001b[1;32m    140\u001b[0m             \u001b[0mPipelineName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/workflow/pipeline.py\u001b[0m in \u001b[0;36mdefinition\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdefinition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[0;34m\"\"\"Converts a request structure to string representation for workflow service calls.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         \u001b[0mrequest_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    302\u001b[0m         request_dict[\"PipelineExperimentConfig\"] = interpolate(\n\u001b[1;32m    303\u001b[0m             \u001b[0mrequest_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"PipelineExperimentConfig\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/workflow/pipeline.py\u001b[0m in \u001b[0;36mto_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline_experiment_config\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;34m\"Steps\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist_to_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         }\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/workflow/utilities.py\u001b[0m in \u001b[0;36mlist_to_request\u001b[0;34m(entities)\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEntity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mrequest_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mStepCollection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mrequest_dicts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest_dicts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/workflow/steps.py\u001b[0m in \u001b[0;36mto_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mto_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mRequestType\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;34m\"\"\"Gets the request structure for `ConfigurableRetryStep`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mstep_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_policies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0mstep_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"RetryPolicies\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resolve_retry_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_policies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/workflow/steps.py\u001b[0m in \u001b[0;36mto_request\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;34m\"Name\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;34m\"Type\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0;34m\"Arguments\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marguments\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         }\n\u001b[1;32m    105\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdepends_on\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/workflow/steps.py\u001b[0m in \u001b[0;36marguments\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    381\u001b[0m                 container_defs=self.model.prepare_container_def(\n\u001b[1;32m    382\u001b[0m                     \u001b[0minstance_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstance_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m                     \u001b[0maccelerator_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m                 ),\n\u001b[1;32m    385\u001b[0m                 \u001b[0mvpc_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvpc_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/model.py\u001b[0m in \u001b[0;36mprepare_container_def\u001b[0;34m(self, instance_type, accelerator_type)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \"\"\"\n\u001b[1;32m    405\u001b[0m         deploy_key_prefix = fw_utils.model_code_key_prefix(\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_uri\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m         )\n\u001b[1;32m    408\u001b[0m         \u001b[0mdeploy_env\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/fw_utils.py\u001b[0m in \u001b[0;36mmodel_code_key_prefix\u001b[0;34m(code_location_key_prefix, model_name, image)\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mkey\u001b[0m \u001b[0mprefix\u001b[0m \u001b[0mto\u001b[0m \u001b[0mbe\u001b[0m \u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0muploading\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m     \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m     \u001b[0mtraining_job_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_from_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcode_location_key_prefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_name\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtraining_job_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/utils.py\u001b[0m in \u001b[0;36mname_from_image\u001b[0;34m(image, max_length)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mmax_length\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mMaximum\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mresulting\u001b[0m \u001b[0mstring\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdefault\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m63\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \"\"\"\n\u001b[0;32m---> 57\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mname_from_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_name_from_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/utils.py\u001b[0m in \u001b[0;36mbase_name_from_image\u001b[0;34m(image)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAlgorithm\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mextracted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mimage\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     \"\"\"\n\u001b[0;32m---> 98\u001b[0;31m     \u001b[0mm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"^(.+/)?([^:/]+)(:[^:]+)?$\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m     \u001b[0malgo_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0malgo_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/re.py\u001b[0m in \u001b[0;36mmatch\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    170\u001b[0m     \"\"\"Try to apply the pattern at the start of the string, returning\n\u001b[1;32m    171\u001b[0m     a match object, or None if no match was found.\"\"\"\n\u001b[0;32m--> 172\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfullmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "pipeline.upsert(role_arn=sagemaker_role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This pipeline is not associated with a Pipeline in SageMaker. Please invoke create() first before attempting to invoke start().",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-3de8c59221de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mstart_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mstart_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstart_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescribe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/workflow/pipeline.py\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self, parameters, execution_display_name, execution_description, parallelism_config)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexists\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             raise ValueError(\n\u001b[0;32m--> 281\u001b[0;31m                 \u001b[0;34m\"This pipeline is not associated with a Pipeline in SageMaker. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m                 \u001b[0;34m\"Please invoke create() first before attempting to invoke start().\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m             )\n",
      "\u001b[0;31mValueError\u001b[0m: This pipeline is not associated with a Pipeline in SageMaker. Please invoke create() first before attempting to invoke start()."
     ]
    }
   ],
   "source": [
    "start_response = pipeline.start()\n",
    "start_response.wait()\n",
    "start_response.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
