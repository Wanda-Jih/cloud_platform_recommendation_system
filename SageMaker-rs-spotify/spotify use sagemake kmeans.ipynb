{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3610137a",
   "metadata": {},
   "source": [
    "## Building Music Recommendation System using Spotify Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74673dfa",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "bdc9ae06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success - the MySageMakerInstance is in the us-east-1 region. You will use the 811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:latest container for your SageMaker endpoint.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import boto3, re, sys, math, json, os, sagemaker, urllib.request\n",
    "from sagemaker import get_execution_role\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "from time import gmtime, strftime\n",
    "from sagemaker.predictor import csv_serializer\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import plotly.express as px \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Define IAM role\n",
    "role = get_execution_role()\n",
    "prefix = 'new_kmeans/'\n",
    "my_region = boto3.session.Session().region_name # set the region of the instance\n",
    "\n",
    "# this line automatically looks for the XGBoost image URI and builds an XGBoost container.\n",
    "xgboost_container = sagemaker.image_uris.retrieve(\"xgboost\", my_region, \"latest\")\n",
    "\n",
    "print(\"Success - the MySageMakerInstance is in the \" + my_region + \" region. You will use the \" + xgboost_container + \" container for your SageMaker endpoint.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953b93c6",
   "metadata": {},
   "source": [
    "### Create bucket on s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e6a42641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 bucket created successfully\n"
     ]
    }
   ],
   "source": [
    "bucket_name = 'bucket-rs-spotify' # <--- Your bucket name cannot contain capital letters.\n",
    "s3 = boto3.resource('s3')\n",
    "try:\n",
    "    if  my_region == 'us-east-1':\n",
    "      s3.create_bucket(Bucket=bucket_name)\n",
    "    else: \n",
    "      s3.create_bucket(Bucket=bucket_name, CreateBucketConfiguration={ 'LocationConstraint': my_region })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e2aade",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "513a7af7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: Data loaded into dataframe.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    data = pd.read_csv(\"./data.csv\")\n",
    "    print('Success: Data loaded into dataframe.')\n",
    "except Exception as e:\n",
    "    print('Data load error: ',e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208679f7",
   "metadata": {},
   "source": [
    "### Upload data to s3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "a37482a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'train/data.csv')).upload_file('data.csv')\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/train'.format(bucket_name, prefix), content_type='csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88962abd",
   "metadata": {},
   "source": [
    "### Clustering Songs with K-Means\n",
    "Use SageMaker in-built model - KMeans\n",
    "Upload built KMeans model to s3 bucket "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "140ea36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering songs with kmeans\n",
    "from sagemaker import KMeans\n",
    "\n",
    "n_clusters = 20\n",
    "kmeans = KMeans(\n",
    "            k=n_clusters,\n",
    "            n_jobs=4,\n",
    "            role=role,\n",
    "            instance_count=1,\n",
    "            instance_type=\"ml.c4.xlarge\",\n",
    "            output_path=\"s3://\" + bucket_name + \"/kmeans/\" )                                                                      \n",
    "                                                                            \n",
    "song_cluster_pipeline = Pipeline([('scaler', StandardScaler()), \n",
    "                                  ('kmeans', kmeans)\n",
    "                                 ], verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "5e24ef75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n",
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-02-26 20:08:54 Starting - Starting the training job...\n",
      "2022-02-26 20:08:59 Starting - Launching requested ML instancesProfilerReport-1645906134: InProgress\n",
      ".........\n",
      "2022-02-26 20:10:37 Starting - Preparing the instances for training......\n",
      "2022-02-26 20:11:52 Downloading - Downloading input data\n",
      "2022-02-26 20:11:52 Training - Downloading the training image.....\u001b[34mDocker entrypoint called with argument(s): train\u001b[0m\n",
      "\u001b[34mRunning default environment configuration script\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] Reading default configuration from /opt/amazon/lib/python3.7/site-packages/algorithm/resources/default-input.json: {'init_method': 'random', 'mini_batch_size': '5000', 'epochs': '1', 'extra_center_factor': 'auto', 'local_lloyd_max_iter': '300', 'local_lloyd_tol': '0.0001', 'local_lloyd_init_method': 'kmeans++', 'local_lloyd_num_trials': 'auto', 'half_life_time_size': '0', 'eval_metrics': '[\"msd\"]', 'force_dense': 'true', '_disable_wait_to_read': 'false', '_enable_profiler': 'false', '_kvstore': 'auto', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_num_slices': '1', '_tuning_objective_metric': ''}\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'feature_dim': '15', 'k': '20', 'force_dense': 'True'}\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] Final configuration: {'init_method': 'random', 'mini_batch_size': '5000', 'epochs': '1', 'extra_center_factor': 'auto', 'local_lloyd_max_iter': '300', 'local_lloyd_tol': '0.0001', 'local_lloyd_init_method': 'kmeans++', 'local_lloyd_num_trials': 'auto', 'half_life_time_size': '0', 'eval_metrics': '[\"msd\"]', 'force_dense': 'True', '_disable_wait_to_read': 'false', '_enable_profiler': 'false', '_kvstore': 'auto', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_num_slices': '1', '_tuning_objective_metric': '', 'feature_dim': '15', 'k': '20'}\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 WARNING 139985502349120] Loggers have already been setup.\u001b[0m\n",
      "\u001b[34mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] Using default worker.\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] Loaded iterator creator application/x-recordio-protobuf for content type ('application/x-recordio-protobuf', '1.0')\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] Create Store: local\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] nvidia-smi: took 0.035 seconds to run.\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] Checkpoint loading and saving are disabled.\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] Setting up with params: {'init_method': 'random', 'mini_batch_size': '5000', 'epochs': '1', 'extra_center_factor': 'auto', 'local_lloyd_max_iter': '300', 'local_lloyd_tol': '0.0001', 'local_lloyd_init_method': 'kmeans++', 'local_lloyd_num_trials': 'auto', 'half_life_time_size': '0', 'eval_metrics': '[\"msd\"]', 'force_dense': 'True', '_disable_wait_to_read': 'false', '_enable_profiler': 'false', '_kvstore': 'auto', '_log_level': 'info', '_num_gpus': 'auto', '_num_kv_servers': '1', '_num_slices': '1', '_tuning_objective_metric': '', 'feature_dim': '15', 'k': '20'}\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] 'extra_center_factor' was set to 'auto', evaluated to 10.\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:34 INFO 139985502349120] number of center slices 1\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1645906354.7467017, \"EndTime\": 1645906354.7467632, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"Meta\": \"init_train_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 5000.0, \"count\": 1, \"min\": 5000, \"max\": 5000}, \"Total Batches Seen\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Max Records Seen Between Resets\": {\"sum\": 5000.0, \"count\": 1, \"min\": 5000, \"max\": 5000}, \"Max Batches Seen Between Resets\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Reset Count\": {\"sum\": 0.0, \"count\": 1, \"min\": 0, \"max\": 0}, \"Number of Records Since Last Reset\": {\"sum\": 5000.0, \"count\": 1, \"min\": 5000, \"max\": 5000}, \"Number of Batches Since Last Reset\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}}}\u001b[0m\n",
      "\u001b[34m[2022-02-26 20:12:34.750] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 0, \"duration\": 89, \"num_examples\": 1, \"num_bytes\": 420000}\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] Iter 10: Short term msd 1595673091.202110. Long term msd 1681465202.774878\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] Iter 20: Short term msd 992779741.219129. Long term msd 1180245440.362972\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] Iter 30: Short term msd 1443916864.861613. Long term msd 1536417913.272264\u001b[0m\n",
      "\u001b[34m[2022-02-26 20:12:35.534] [tensorio] [info] epoch_stats={\"data_pipeline\": \"/opt/ml/input/data/train\", \"epoch\": 1, \"duration\": 783, \"num_examples\": 35, \"num_bytes\": 14334852}\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] processed a total of 170653 examples\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] #progress_metric: host=algo-1, completed 100.0 % of epochs\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1645906354.7504637, \"EndTime\": 1645906355.5357413, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\", \"epoch\": 0, \"Meta\": \"training_data_iter\"}, \"Metrics\": {\"Total Records Seen\": {\"sum\": 175653.0, \"count\": 1, \"min\": 175653, \"max\": 175653}, \"Total Batches Seen\": {\"sum\": 36.0, \"count\": 1, \"min\": 36, \"max\": 36}, \"Max Records Seen Between Resets\": {\"sum\": 170653.0, \"count\": 1, \"min\": 170653, \"max\": 170653}, \"Max Batches Seen Between Resets\": {\"sum\": 35.0, \"count\": 1, \"min\": 35, \"max\": 35}, \"Reset Count\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"Number of Records Since Last Reset\": {\"sum\": 170653.0, \"count\": 1, \"min\": 170653, \"max\": 170653}, \"Number of Batches Since Last Reset\": {\"sum\": 35.0, \"count\": 1, \"min\": 35, \"max\": 35}}}\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] #throughput_metric: host=algo-1, train throughput=217272.232089557 records/second\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 WARNING 139985502349120] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] shrinking 200 centers into 20\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] local kmeans attempt #0. Current mean square distance 50138208.000000\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] local kmeans attempt #1. Current mean square distance 48500244.000000\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] local kmeans attempt #2. Current mean square distance 44128436.000000\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] local kmeans attempt #3. Current mean square distance 48091676.000000\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] local kmeans attempt #4. Current mean square distance 48056496.000000\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:35 INFO 139985502349120] local kmeans attempt #5. Current mean square distance 47398104.000000\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] local kmeans attempt #6. Current mean square distance 48413232.000000\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] local kmeans attempt #7. Current mean square distance 46815112.000000\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] local kmeans attempt #8. Current mean square distance 48169428.000000\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] local kmeans attempt #9. Current mean square distance 46243076.000000\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] finished shrinking process. Mean Square Distance = 44128436\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] #quality_metric: host=algo-1, train msd <loss>=44128436.0\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] predict compute msd took: 38.5178%, (0.302677 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] gradient: cluster size  took: 23.6462%, (0.185815 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] compute all data-center distances: inner product took: 15.6284%, (0.122809 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] gradient: cluster center took: 6.7569%, (0.053097 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] compute all data-center distances: point norm took: 3.0272%, (0.023788 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] gradient: one_hot took: 2.7021%, (0.021234 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] update state and report convergance took: 2.5667%, (0.020169 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] batch data loading with context took: 2.3006%, (0.018078 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] collect from kv store took: 2.2035%, (0.017316 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] splitting centers key-value pair took: 1.6298%, (0.012807 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] compute all data-center distances: center norm took: 0.8677%, (0.006819 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] predict minus dist took: 0.1211%, (0.000952 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] update set-up time took: 0.0320%, (0.000251 secs)\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] TOTAL took: 0.7858116626739502\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1645906354.6598496, \"EndTime\": 1645906356.1978822, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 77.85558700561523, \"count\": 1, \"min\": 77.85558700561523, \"max\": 77.85558700561523}, \"epochs\": {\"sum\": 1.0, \"count\": 1, \"min\": 1, \"max\": 1}, \"update.time\": {\"sum\": 785.0551605224609, \"count\": 1, \"min\": 785.0551605224609, \"max\": 785.0551605224609}, \"_shrink.time\": {\"sum\": 658.3735942840576, \"count\": 1, \"min\": 658.3735942840576, \"max\": 658.3735942840576}, \"finalize.time\": {\"sum\": 660.1803302764893, \"count\": 1, \"min\": 660.1803302764893, \"max\": 660.1803302764893}, \"model.serialize.time\": {\"sum\": 1.5726089477539062, \"count\": 1, \"min\": 1.5726089477539062, \"max\": 1.5726089477539062}}}\u001b[0m\n",
      "\u001b[34m[02/26/2022 20:12:36 INFO 139985502349120] Test data is not provided.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1645906356.1980038, \"EndTime\": 1645906356.198943, \"Dimensions\": {\"Algorithm\": \"AWS/KMeansWebscale\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 15.275001525878906, \"count\": 1, \"min\": 15.275001525878906, \"max\": 15.275001525878906}, \"totaltime\": {\"sum\": 1617.5432205200195, \"count\": 1, \"min\": 1617.5432205200195, \"max\": 1617.5432205200195}}}\u001b[0m\n",
      "\n",
      "2022-02-26 20:12:50 Uploading - Uploading generated training model\n",
      "2022-02-26 20:12:50 Completed - Training job completed\n",
      "Training seconds: 73\n",
      "Billable seconds: 73\n"
     ]
    }
   ],
   "source": [
    "X = data.select_dtypes(np.number)\n",
    "number_cols = list(X.columns)\n",
    "\n",
    "new_data = data\n",
    "new_data = new_data.drop(['artists', 'id', 'name', 'release_date'], 1)\n",
    "train_data = new_data.values.astype(\"float32\")\n",
    "\n",
    "song_cluster_pipeline.steps[1][1].fit(song_cluster_pipeline.steps[1][1].record_set(train_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "f901be2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Defaulting to the only supported framework/algorithm version: 1. Ignoring framework/algorithm version: 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------!"
     ]
    }
   ],
   "source": [
    "kmeans_song_predictor = song_cluster_pipeline.steps[1][1].deploy(initial_instance_count=1, instance_type=\"ml.m4.xlarge\")\n",
    "# ml.t2 medium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "30c55511",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data1 = train_data[0:50000]\n",
    "train_data2 = train_data[50000:100000]\n",
    "train_data3 = train_data[100000:150000]\n",
    "train_data4 = train_data[150000:]\n",
    "result1=kmeans_song_predictor.predict(train_data1)\n",
    "result2=kmeans_song_predictor.predict(train_data2)\n",
    "result3=kmeans_song_predictor.predict(train_data3)\n",
    "result4=kmeans_song_predictor.predict(train_data4)\n",
    "\n",
    "\n",
    "result = result1 + result2 + result3 + result4\n",
    "\n",
    "data['cluster_label'] = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3f81566f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('kmeans', KMeans(n_clusters=20, verbose=False))])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "song_cluster_pipeline = Pipeline([('scaler', StandardScaler()), \n",
    "                                  ('kmeans', KMeans(n_clusters=20, \n",
    "                                   verbose=False))\n",
    "                                 ], verbose=False)\n",
    "\n",
    "\n",
    "song_cluster_pipeline.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60cefc8",
   "metadata": {},
   "source": [
    "### Build Recommender System\n",
    "* Based on the analysis and visualizations, it’s clear that similar genres tend to have data points that are located close to each other while similar types of songs are also clustered together.\n",
    "* This observation makes perfect sense. Similar genres will sound similar and will come from similar time periods while the same can be said for songs within those genres. We can use this idea to build a recommendation system by taking the data points of the songs a user has listened to and recommending songs corresponding to nearby data points.\n",
    "* Spotipy is a Python client for the Spotify Web API that makes it easy for developers to fetch data and query Spotify’s catalog for songs. You have to install using pip install spotipy\n",
    "* After installing Spotipy, you will need to create an app on the Spotify Developer’s page and save your Client ID and secret key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "75415ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "from collections import defaultdict\n",
    "\n",
    "sp = spotipy.Spotify(auth_manager=SpotifyClientCredentials(client_id= \"01619dbbc94443d5a0828c00a490ab72\",\n",
    "                                                           client_secret=\"f569c086f1934fa08806682496564ddb\"))\n",
    " \n",
    "def find_song(name, year):\n",
    "    song_data = defaultdict()\n",
    "    results = sp.search(q= 'track: {} year: {}'.format(name,year), limit=1)\n",
    "    if results['tracks']['items'] == []:\n",
    "        return None\n",
    "\n",
    "    results = results['tracks']['items'][0]\n",
    "    track_id = results['id']\n",
    "    audio_features = sp.audio_features(track_id)[0]\n",
    "\n",
    "    song_data['name'] = [name]\n",
    "    song_data['year'] = [year]\n",
    "    song_data['explicit'] = [int(results['explicit'])]\n",
    "    song_data['duration_ms'] = [results['duration_ms']]\n",
    "    song_data['popularity'] = [results['popularity']]\n",
    "\n",
    "    for key, value in audio_features.items():\n",
    "        song_data[key] = value\n",
    "\n",
    "    return pd.DataFrame(song_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "5335ac18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn.metrics import euclidean_distances\n",
    "from scipy.spatial.distance import cdist\n",
    "import difflib\n",
    "\n",
    "number_cols = ['valence', 'year', 'acousticness', 'danceability', 'duration_ms', 'energy', 'explicit',\n",
    " 'instrumentalness', 'key', 'liveness', 'loudness', 'mode', 'popularity', 'speechiness', 'tempo']\n",
    "\n",
    "\n",
    "def get_song_data(song, spotify_data):\n",
    "    \n",
    "    try:\n",
    "        song_data = spotify_data[(spotify_data['name'] == song['name']) \n",
    "                                & (spotify_data['year'] == song['year'])].iloc[0]\n",
    "        return song_data\n",
    "    \n",
    "    except IndexError:\n",
    "        return find_song(song['name'], song['year'])\n",
    "        \n",
    "\n",
    "def get_mean_vector(song_list, spotify_data):\n",
    "    \n",
    "    song_vectors = []\n",
    "    \n",
    "    for song in song_list:\n",
    "        song_data = get_song_data(song, spotify_data)\n",
    "        if song_data is None:\n",
    "            print('Warning: {} does not exist in Spotify or in database'.format(song['name']))\n",
    "            continue\n",
    "        song_vector = song_data[number_cols].values\n",
    "        song_vectors.append(song_vector)  \n",
    "    \n",
    "    song_matrix = np.array(list(song_vectors))\n",
    "    return np.mean(song_matrix, axis=0)\n",
    "\n",
    "\n",
    "def flatten_dict_list(dict_list):\n",
    "    \n",
    "    flattened_dict = defaultdict()\n",
    "    for key in dict_list[0].keys():\n",
    "        flattened_dict[key] = []\n",
    "    \n",
    "    for dictionary in dict_list:\n",
    "        for key, value in dictionary.items():\n",
    "            flattened_dict[key].append(value)\n",
    "            \n",
    "    return flattened_dict\n",
    "\n",
    "\n",
    "def recommend_songs( song_list, spotify_data, n_songs=10):\n",
    "    \n",
    "    metadata_cols = ['name', 'year', 'artists']\n",
    "    song_dict = flatten_dict_list(song_list)\n",
    "    \n",
    "    song_center = get_mean_vector(song_list, spotify_data)\n",
    "    scaler = song_cluster_pipeline.steps[0][1]\n",
    "    scaled_data = scaler.transform(spotify_data[number_cols])\n",
    "    scaled_song_center = scaler.transform(song_center.reshape(1, -1))\n",
    "    distances = cdist(scaled_song_center, scaled_data, 'cosine')\n",
    "    index = list(np.argsort(distances)[:, :n_songs][0])\n",
    "    \n",
    "    rec_songs = spotify_data.iloc[index]\n",
    "    rec_songs = rec_songs[~rec_songs['name'].isin(song_dict['name'])]\n",
    "    return rec_songs[metadata_cols].to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "15252786",
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_songs_data = recommend_songs([{'name': 'Come As You Are', 'year':1991},\n",
    "                {'name': 'Smells Like Teen Spirit', 'year': 1991},\n",
    "                {'name': 'Lithium', 'year': 1992},\n",
    "                {'name': 'All Apologies', 'year': 1993},\n",
    "                {'name': 'Stay Away', 'year': 1993},\n",
    "                {'name': 'Billetes Azules (with J Balvin)', 'year': 2020}],  data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6917f3e",
   "metadata": {},
   "source": [
    "### Upload the result to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a182901",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('recommend_songs_result.csv', \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow({\"name\", \"year\", \"artists\"})\n",
    "    for line in recommend_songs_data:\n",
    "        newList = [str(line[\"name\"]) , str(line[\"year\"]), str(line[\"artists\"])]\n",
    "        writer.writerow(newList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b3864b",
   "metadata": {},
   "outputs": [],
   "source": [
    "boto3.Session().resource('s3').Bucket(bucket_name).Object(os.path.join(prefix, 'result/recommend_songs_result.csv')).upload_file('recommend_songs_result.csv')\n",
    "s3_input_train = sagemaker.inputs.TrainingInput(s3_data='s3://{}/{}/result'.format(bucket_name, prefix), content_type='csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
